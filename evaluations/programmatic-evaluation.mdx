---
title: Programmatic Evaluation
---

Murnitur AI allows developers to run programmatic evaluations, providing a flexible and automated way to assess Large Language Models (LLMs) through code.

**Note**: Before running programmatic evaluations, you need to set up the Murnitur SDK with your API key. Detailed instructions on how to set up the SDK can be found [here](/installation).

## Example

<CodeGroup>

```python python

import murnitur
from murnitur.loaders import Loader
from murnitur.annotation.eval import Evaluation

# initialize evaluation
murnitur.set_api_key("mt-ey...")
key = "sk-..."

eval = Evaluation(openai_api_key=key, murnix_api_key=murnitur.get_api_key())

# Load datasets from csv
data = Loader.load_csv("https://murnitur.github.io/murnix-kube/sample-dataset.csv")

results = eval.run_suite(metrics=["hallucination", "pii"], dataset=data)

print(results)

```

</CodeGroup>
Each run is saved in the UI. You can view the results on the **AI Evaluations** page.

## Creating dataset

**_Side Note_**: If you are creating a dataset from a dictionary, use the following data structure:

<CodeGroup>
```python python
dataset = [
    {
        "input": "",
        "output": "",
        "ground_truth": "",
        "context": [""],
        "retrieval_context": [""],
    }
]

````

</CodeGroup>

## Loading dataset

Use any of the following functions to load the dataset:

<CodeGroup>
```python python
from murnitur.loaders import Loader

dataset_from_json = Loader.load_json(path_to_json_file)

dataset_from_csv = Loader.load_csv(path_to_csv_document)

````

</CodeGroup>

## Extra Params

Extra parameters that can be passed to `run_suite`

- `save_output`: A boolean. Set to false if you do not want to store the results in Murnitur AI.
- `async_mode`: Indicates whether the requests should run asynchronously.

## Evaluation Metrics

We currently support the following evaluation metrics:

- `hallucination`: Measures the accuracy of the model's output, identifying any fabricated or incorrect information.
- `faithfulness`: Evaluates how well the model's output adheres to the provided context or source material.
- `relevancy`: Assesses the relevance of the model's response to the input query or prompt.
- `bias`: Detects any unfair or discriminatory tendencies in the model's predictions.
- `context-relevancy`: Determines how relevant the output is in relation to the given context.
- `context-precision`: Measures the precision of the model's output within the specific context provided.
- `toxicity`: Identifies harmful or offensive language in the model's responses.
- `summarization`: Evaluates the quality and accuracy of summaries generated by the model.
- `pii`: Detects the presence of personally identifiable information in the model's output.

```

```

```

```
