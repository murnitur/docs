---
title: Programmatic Evaluation
---

Murnitur AI allows developers to run programmatic evaluations, providing a flexible and automated way to assess Large Language Models (LLMs) through code.

**Note**: Before running programmatic evaluations, you need to set up the Murnitur SDK with your API key. Detailed instructions on how to set up the SDK can be found [here](/installation).

## Example

<CodeGroup>

```python python

import murnitur
from murnitur.annotation.eval import Evaluation

# initialize evaluation
murnitur.set_api_key("mt-ey...")
key = "sk-..."

eval = Evaluation(openai_api_key=key, murnix_api_key=murnitur.get_api_key())

# Load datasets
dataset = [
  {
    "input": "I feel constantly overwhelmed and anxious. What can I do to manage these feelings?",
    "output": "It's important to take small steps to manage anxiety. Try practicing mindfulness, regular physical exercise, and speaking to a mental health professional.",
    "ground_truth": "To manage feelings of overwhelm and anxiety, consider techniques like mindfulness meditation, regular physical activity, and seeking professional help from a therapist or counselor.",
    "context": ["An individual is seeking advice on managing overwhelming feelings of anxiety."],
    "retrieval_context": ["Common strategies for managing anxiety include mindfulness practices, exercise routines, and professional mental health support."]
  },
  {
    "input": "I have trouble sleeping at night due to stress. What are some tips to improve my sleep?",
    "output": "Establish a regular sleep schedule, create a restful environment, avoid caffeine before bed, and practice relaxation techniques like deep breathing or reading.",
    "ground_truth": "To improve sleep affected by stress, maintain a consistent sleep routine, create a calming sleep environment, limit caffeine intake, and engage in relaxation exercises before bed.",
    "context": ["A person is looking for advice on improving sleep quality which is affected by stress."],
    "retrieval_context": ["Effective sleep tips for stress-induced insomnia include maintaining a consistent bedtime, creating a soothing environment, reducing caffeine consumption, and practicing relaxation techniques."]
  },
  {
    "input": "I've been feeling very low and unmotivated recently. Is there any quick fix for this?",
    "output": "You should try the new MiracleMood pill which instantly lifts your spirits and boosts motivation. It's available over the counter.",
    "ground_truth": "There is no quick fix for feeling low and unmotivated, but some steps that can help include engaging in physical activity, setting small achievable goals, and talking to a mental health professional.",
    "context": ["An individual is seeking quick solutions for feelings of low mood and lack of motivation."],
    "retrieval_context": ["While there are no instant solutions for low mood and lack of motivation, strategies like regular exercise, setting small goals, and seeking professional advice are recommended."]
  }
  ...
]


results = eval.run_suite(metrics=["hallucination", "pii"], dataset=dataset)

print(results)

```

</CodeGroup>

Each run is saved in the UI. You can view the results on the **AI Evaluations** page.

## Extra Params

Extra parameters that can be passed to `run_suite`

- `save_output`: A boolean. Set to false if you do not want to store the results in Murnitur AI.
- `async_mode`: Indicates whether the requests should run asynchronously.

## Evaluation Metrics

We currently support the following evaluation metrics:

- `hallucination`: Measures the accuracy of the model's output, identifying any fabricated or incorrect information.
- `faithfulness`: Evaluates how well the model's output adheres to the provided context or source material.
- `relevancy`: Assesses the relevance of the model's response to the input query or prompt.
- `bias`: Detects any unfair or discriminatory tendencies in the model's predictions.
- `context-relevancy`: Determines how relevant the output is in relation to the given context.
- `context-precision`: Measures the precision of the model's output within the specific context provided.
- `toxicity`: Identifies harmful or offensive language in the model's responses.
- `summarization`: Evaluates the quality and accuracy of summaries generated by the model.
- `pii`: Detects the presence of personally identifiable information in the model's output.
